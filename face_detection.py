import cv2
import pickle
import numpy as np
import torch
from PIL import Image
from facenet_pytorch import MTCNN, InceptionResnetV1
import datetime, csv

# ======================
# Load dá»¯ liá»‡u Ä‘Ã£ train (pkl chá»©a embeddings + names)
# ======================
with open("encodings/face_encodings.pkl", "rb") as f:
    data = pickle.load(f)

TOLERANCE = data.get("tolerance", 0.48)  # ngÆ°á»¡ng cosine similarity
device = "mps" if torch.backends.mps.is_available() else (
    "cuda" if torch.cuda.is_available() else "cpu"
)


# ======================
# Load model (pretrained hoáº·c fine-tuned náº¿u cÃ³ .pth)
# ======================
model = InceptionResnetV1(pretrained="vggface2", classify=False).to(device)

# Náº¿u cÃ³ model fine-tune thÃ¬ load, náº¿u khÃ´ng thÃ¬ dÃ¹ng pretrained
try:
    model.load_state_dict(torch.load("models/facenet_triplet.pth", map_location=device))
    print("âœ… Loaded fine-tuned model facenet_triplet.pth")
except FileNotFoundError:
    print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y facenet_triplet.pth, dÃ¹ng pretrained FaceNet.")

model.eval()

# Detector MTCNN
mtcnn = MTCNN(image_size=160, margin=20, device="cpu")

# ======================
# HÃ m trÃ­ch embedding
# ======================
def get_embedding(img_bgr):
    rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    pil = Image.fromarray(rgb)
    face = mtcnn(pil)
    if face is None:
        return None
    with torch.no_grad():
        emb = model(face.unsqueeze(0).to(device)).cpu().numpy()[0]
    # chuáº©n hÃ³a L2
    n = np.linalg.norm(emb)
    if n > 0:
        emb = emb / n
    return emb

# ======================
# Ghi Ä‘iá»ƒm danh
# ======================
def mark_attendance(name):
    now = datetime.datetime.now()
    with open("attendance.csv", "a", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([name, now.strftime("%Y-%m-%d"), now.strftime("%H:%M:%S"), "Present"])

# ======================
# Chá»¥p vÃ  nháº­n diá»‡n
# ======================
def capture_and_recognize():
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ KhÃ´ng má»Ÿ Ä‘Æ°á»£c camera")
        return

    print("ğŸ“¸ Nháº¥n SPACE Ä‘á»ƒ chá»¥p, ESC Ä‘á»ƒ thoÃ¡t")
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        cv2.imshow("Press SPACE to Capture", frame)
        key = cv2.waitKey(1) & 0xFF

        if key == 27:  # ESC thoÃ¡t
            break
        elif key == 32:  # SPACE Ä‘á»ƒ chá»¥p
            emb = get_embedding(frame)
            if emb is None:
                print("âš ï¸ KhÃ´ng phÃ¡t hiá»‡n khuÃ´n máº·t. Vui lÃ²ng thá»­ láº¡i.")
                continue

            sims = np.dot(data["encodings"], emb)  # cosine similarity vÃ¬ Ä‘Ã£ L2 normalize
            idx = np.argmax(sims)
            best_score = sims[idx]

            print(f"[DEBUG] Similarity scores: {sims}")   # in toÃ n bá»™ Ä‘iá»ƒm sá»‘
            print(f"[DEBUG] Best match: {data['names'][idx]} vá»›i score={best_score:.4f}")

            if best_score > TOLERANCE:
                name = data["names"][idx]
                print(f"âœ… Äiá»ƒm danh thÃ nh cÃ´ng: {name}")
                mark_attendance(name)
            else:
                print("âŒ KhÃ´ng khá»›p trong cÆ¡ sá»Ÿ dá»¯ liá»‡u. NgÆ°á»i dÃ¹ng chÆ°a Ä‘Äƒng kÃ½.")

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    capture_and_recognize()
